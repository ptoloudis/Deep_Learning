{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3IiEderZ8AY"
      },
      "source": [
        "# Import the nessasary librarys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VaCL9_t4Z5M8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import pandas as pd \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F \n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import torch.backends.cudnn as cudnn\n",
        "import scipy.sparse as sp\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoI4OzgGajFC"
      },
      "source": [
        "# The function for the evaluate.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vGNK3U0warUP"
      },
      "outputs": [],
      "source": [
        "def hit(gt_item, pred_items):\n",
        "\tif gt_item in pred_items:\n",
        "\t\treturn 1\n",
        "\treturn 0\n",
        "\n",
        "\n",
        "def ndcg(gt_item, pred_items):\n",
        "\tif gt_item in pred_items:\n",
        "\t\tindex = pred_items.index(gt_item)\n",
        "\t\treturn np.reciprocal(np.log2(index+2))\n",
        "\treturn 0\n",
        "\n",
        "\n",
        "def metrics(model, test_loader, top_k):\n",
        "\tHR, NDCG = [], []\n",
        "\n",
        "\tfor user, item, label in test_loader:\n",
        "\t\tuser = user.cuda()\n",
        "\t\titem = item.cuda()\n",
        "\n",
        "\t\tpredictions = model(user, item)\n",
        "\t\t_, indices = torch.topk(predictions, top_k)\n",
        "\t\trecommends = torch.take(\n",
        "\t\t\t\titem, indices).cpu().numpy().tolist()\n",
        "\n",
        "\t\tgt_item = item[0].item()\n",
        "\t\tHR.append(hit(gt_item, recommends))\n",
        "\t\tNDCG.append(ndcg(gt_item, recommends))\n",
        "\n",
        "\treturn np.mean(HR), np.mean(NDCG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FV_cfGXxawoC"
      },
      "source": [
        "# The code for the model.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "gETLuYHqawM3"
      },
      "outputs": [],
      "source": [
        "class NCF(nn.Module):\n",
        "\tdef __init__(self, user_num, item_num, factor_num, num_layers,\n",
        "\t\t\t\t\tdropout, model, GMF_model=None, MLP_model=None):\n",
        "\t\tsuper(NCF, self).__init__()\n",
        "\t\t\"\"\"\n",
        "\t\tuser_num: number of users;\n",
        "\t\titem_num: number of items;\n",
        "\t\tfactor_num: number of predictive factors;\n",
        "\t\tnum_layers: the number of layers in MLP model;\n",
        "\t\tdropout: dropout rate between fully connected layers;\n",
        "\t\tmodel: 'MLP', 'GMF', 'NeuMF-end', and 'NeuMF-pre';\n",
        "\t\tGMF_model: pre-trained GMF weights;\n",
        "\t\tMLP_model: pre-trained MLP weights.\n",
        "\t\t\"\"\"\t\t\n",
        "\t\tself.dropout = dropout\n",
        "\t\tself.model = model\n",
        "\t\tself.GMF_model = GMF_model\n",
        "\t\tself.MLP_model = MLP_model\n",
        "\n",
        "\t\tself.embed_user_GMF = nn.Embedding(user_num, factor_num)\n",
        "\t\tself.embed_item_GMF = nn.Embedding(item_num, factor_num)\n",
        "\t\tself.embed_user_MLP = nn.Embedding(\n",
        "\t\t\t\tuser_num, factor_num * (2 ** (num_layers - 1)))\n",
        "\t\tself.embed_item_MLP = nn.Embedding(\n",
        "\t\t\t\titem_num, factor_num * (2 ** (num_layers - 1)))\n",
        "\n",
        "\t\tMLP_modules = []\n",
        "\t\tfor i in range(num_layers):\n",
        "\t\t\tinput_size = factor_num * (2 ** (num_layers - i))\n",
        "\t\t\tMLP_modules.append(nn.Dropout(p=self.dropout))\n",
        "\t\t\tMLP_modules.append(nn.Linear(input_size, input_size//2))\n",
        "\t\t\tMLP_modules.append(nn.ReLU())\n",
        "\t\tself.MLP_layers = nn.Sequential(*MLP_modules)\n",
        "\n",
        "\t\tif self.model in ['MLP', 'GMF']:\n",
        "\t\t\tpredict_size = factor_num \n",
        "\t\telse:\n",
        "\t\t\tpredict_size = factor_num * 2\n",
        "\t\tself.predict_layer = nn.Linear(predict_size, 1)\n",
        "\n",
        "\t\tself._init_weight_()\n",
        "\n",
        "\tdef _init_weight_(self):\n",
        "\t\t\"\"\" We leave the weights initialization here. \"\"\"\n",
        "\t\tif not self.model == 'NeuMF-pre':\n",
        "\t\t\tnn.init.normal_(self.embed_user_GMF.weight, std=0.01)\n",
        "\t\t\tnn.init.normal_(self.embed_user_MLP.weight, std=0.01)\n",
        "\t\t\tnn.init.normal_(self.embed_item_GMF.weight, std=0.01)\n",
        "\t\t\tnn.init.normal_(self.embed_item_MLP.weight, std=0.01)\n",
        "\n",
        "\t\t\tfor m in self.MLP_layers:\n",
        "\t\t\t\tif isinstance(m, nn.Linear):\n",
        "\t\t\t\t\tnn.init.xavier_uniform_(m.weight)\n",
        "\t\t\tnn.init.kaiming_uniform_(self.predict_layer.weight, \n",
        "\t\t\t\t\t\t\t\t\ta=1, nonlinearity='sigmoid')\n",
        "\n",
        "\t\t\tfor m in self.modules():\n",
        "\t\t\t\tif isinstance(m, nn.Linear) and m.bias is not None:\n",
        "\t\t\t\t\tm.bias.data.zero_()\n",
        "\t\telse:\n",
        "\t\t\t# embedding layers\n",
        "\t\t\tself.embed_user_GMF.weight.data.copy_(\n",
        "\t\t\t\t\t\t\tself.GMF_model.embed_user_GMF.weight)\n",
        "\t\t\tself.embed_item_GMF.weight.data.copy_(\n",
        "\t\t\t\t\t\t\tself.GMF_model.embed_item_GMF.weight)\n",
        "\t\t\tself.embed_user_MLP.weight.data.copy_(\n",
        "\t\t\t\t\t\t\tself.MLP_model.embed_user_MLP.weight)\n",
        "\t\t\tself.embed_item_MLP.weight.data.copy_(\n",
        "\t\t\t\t\t\t\tself.MLP_model.embed_item_MLP.weight)\n",
        "\n",
        "\t\t\t# mlp layers\n",
        "\t\t\tfor (m1, m2) in zip(\n",
        "\t\t\t\tself.MLP_layers, self.MLP_model.MLP_layers):\n",
        "\t\t\t\tif isinstance(m1, nn.Linear) and isinstance(m2, nn.Linear):\n",
        "\t\t\t\t\tm1.weight.data.copy_(m2.weight)\n",
        "\t\t\t\t\tm1.bias.data.copy_(m2.bias)\n",
        "\n",
        "\t\t\t# predict layers\n",
        "\t\t\tpredict_weight = torch.cat([\n",
        "\t\t\t\tself.GMF_model.predict_layer.weight, \n",
        "\t\t\t\tself.MLP_model.predict_layer.weight], dim=1)\n",
        "\t\t\tprecit_bias = self.GMF_model.predict_layer.bias + \\\n",
        "\t\t\t\t\t\tself.MLP_model.predict_layer.bias\n",
        "\n",
        "\t\t\tself.predict_layer.weight.data.copy_(0.5 * predict_weight)\n",
        "\t\t\tself.predict_layer.bias.data.copy_(0.5 * precit_bias)\n",
        "\n",
        "\tdef forward(self, user, item):\n",
        "\t\tif not self.model == 'MLP':\n",
        "\t\t\tembed_user_GMF = self.embed_user_GMF(user)\n",
        "\t\t\tembed_item_GMF = self.embed_item_GMF(item)\n",
        "\t\t\toutput_GMF = embed_user_GMF * embed_item_GMF\n",
        "\t\tif not self.model == 'GMF':\n",
        "\t\t\tembed_user_MLP = self.embed_user_MLP(user)\n",
        "\t\t\tembed_item_MLP = self.embed_item_MLP(item)\n",
        "\t\t\tinteraction = torch.cat((embed_user_MLP, embed_item_MLP), -1)\n",
        "\t\t\toutput_MLP = self.MLP_layers(interaction)\n",
        "\n",
        "\t\tif self.model == 'GMF':\n",
        "\t\t\tconcat = output_GMF\n",
        "\t\telif self.model == 'MLP':\n",
        "\t\t\tconcat = output_MLP\n",
        "\t\telse:\n",
        "\t\t\tconcat = torch.cat((output_GMF, output_MLP), -1)\n",
        "\n",
        "\t\tprediction = self.predict_layer(concat)\n",
        "\t\treturn prediction.view(-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUJWvhXhcD_a"
      },
      "source": [
        "# ***Must to be chance***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IN0hKuE0hfeP"
      },
      "source": [
        "# The code for the config.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "TrDMfOkbA0lM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fda826d7-68ae-4617-9aa4-0bdf7242b738"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XtKBBOU1hn64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f0226ad-1851-40c2-f104-fac0c2aa131d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/deep\n"
          ]
        }
      ],
      "source": [
        "# dataset name \n",
        "dataset = 'u.data'\n",
        "\n",
        "# model name \n",
        "model = 'NeuMF-end'\n",
        "\n",
        "%cd 'drive/MyDrive/Colab Notebooks/deep'\n",
        "model_path = 'model/'\n",
        "\n",
        "train_rating = '{}'.format(dataset)\n",
        "test_negative = '{}.negative'.format(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycKQCrVTbJdL"
      },
      "source": [
        "# The code for the data_utils.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Ak6x53jKbUpk"
      },
      "outputs": [],
      "source": [
        "def load_all(test_num=100):\n",
        "    \"\"\" Load all three files to save time in each epoch. \"\"\"\n",
        "    train_data = pd.read_csv(\n",
        "        train_rating,\n",
        "        sep='\\t', header=None, names=['user', 'item'],\n",
        "        usecols=[0, 1], dtype={0: np.int32, 1: np.int32})\n",
        "\n",
        "    user_num = train_data['user'].max() + 1\n",
        "    item_num = train_data['item'].max() + 1\n",
        "\n",
        "    train_x, test_x, train_y, test_y = train_test_split(train_data['user'], train_data['item'], test_size=0.2,\n",
        "                                                        random_state=42)\n",
        "    train_data = train_data.loc[train_data['user'].isin(train_x) & train_data['item'].isin(train_y)]\n",
        "    test_data = pd.DataFrame({'user': test_x, 'item': test_y})\n",
        "\n",
        "    train_data = train_data.values.tolist()\n",
        "    test_data = test_data.values.tolist()\n",
        "\n",
        "    # Load ratings as a dok matrix\n",
        "    train_mat = sp.dok_matrix((user_num, item_num), dtype=np.float32)\n",
        "    for x in train_data:\n",
        "        train_mat[x[0], x[1]] = 1.0\n",
        "\n",
        "    return train_data, test_data, user_num, item_num, train_mat\n",
        "\n",
        "\n",
        "class NCFData(data.Dataset):\n",
        "\tdef __init__(self, features, \n",
        "\t\t\t\tnum_item, train_mat=None, num_ng=0, is_training=None):\n",
        "\t\tsuper(NCFData, self).__init__()\n",
        "\t\t\"\"\" Note that the labels are only useful when training, we thus \n",
        "\t\t\tadd them in the ng_sample() function.\n",
        "\t\t\"\"\"\n",
        "\t\tself.features_ps = features\n",
        "\t\tself.num_item = num_item\n",
        "\t\tself.train_mat = train_mat\n",
        "\t\tself.num_ng = num_ng\n",
        "\t\tself.is_training = is_training\n",
        "\t\tself.labels = [0 for _ in range(len(features))]\n",
        "\n",
        "\tdef ng_sample(self):\n",
        "\t\tassert self.is_training, 'no need to sampling when testing'\n",
        "\n",
        "\t\tself.features_ng = []\n",
        "\t\tfor x in self.features_ps:\n",
        "\t\t\tu = x[0]\n",
        "\t\t\tfor t in range(self.num_ng):\n",
        "\t\t\t\tj = np.random.randint(self.num_item)\n",
        "\t\t\t\twhile (u, j) in self.train_mat:\n",
        "\t\t\t\t\tj = np.random.randint(self.num_item)\n",
        "\t\t\t\tself.features_ng.append([u, j])\n",
        "\n",
        "\t\tlabels_ps = [1 for _ in range(len(self.features_ps))]\n",
        "\t\tlabels_ng = [0 for _ in range(len(self.features_ng))]\n",
        "\n",
        "\t\tself.features_fill = self.features_ps + self.features_ng\n",
        "\t\tself.labels_fill = labels_ps + labels_ng\n",
        "\n",
        "\tdef __len__(self):\n",
        "\t\treturn (self.num_ng + 1) * len(self.labels)\n",
        "\n",
        "\tdef __getitem__(self, idx):\n",
        "\t\tfeatures = self.features_fill if self.is_training \\\n",
        "\t\t\t\t\telse self.features_ps\n",
        "\t\tlabels = self.labels_fill if self.is_training \\\n",
        "\t\t\t\t\telse self.labels\n",
        "\n",
        "\t\tuser = features[idx][0]\n",
        "\t\titem = features[idx][1]\n",
        "\t\tlabel = labels[idx]\n",
        "\t\treturn user, item ,label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDdF7MbFb0mw"
      },
      "source": [
        "# The code for the main.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-kkSSITcbYj"
      },
      "source": [
        "## The default argument\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hw1_RxmVcgZf"
      },
      "outputs": [],
      "source": [
        "lr =0.001\n",
        "dropout=0.0\n",
        "batch_size=256\n",
        "epochs=20\n",
        "# top_k=10 \n",
        "factor_num=32\n",
        "num_layers=3 \n",
        "num_ng=4\n",
        "test_num_ng=99\n",
        "out=True\n",
        "gpu='0'\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu\n",
        "cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20kQCBOidJ_w"
      },
      "source": [
        "## The code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "EiiZRo70b37y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1e4aa52-34dc-4363-f393-87ce388e9525"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top_K 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The time elapse of epoch 000 is: 00: 00: 14\n",
            "HR: 0.010\tNDCG: 0.010\n",
            "The time elapse of epoch 001 is: 00: 00: 13\n",
            "HR: 0.010\tNDCG: 0.010\n",
            "The time elapse of epoch 002 is: 00: 00: 12\n",
            "HR: 0.015\tNDCG: 0.015\n",
            "The time elapse of epoch 003 is: 00: 00: 12\n",
            "HR: 0.005\tNDCG: 0.005\n",
            "The time elapse of epoch 004 is: 00: 00: 12\n",
            "HR: 0.010\tNDCG: 0.010\n",
            "The time elapse of epoch 005 is: 00: 00: 12\n",
            "HR: 0.010\tNDCG: 0.010\n",
            "The time elapse of epoch 006 is: 00: 00: 11\n",
            "HR: 0.005\tNDCG: 0.005\n",
            "The time elapse of epoch 007 is: 00: 00: 11\n",
            "HR: 0.005\tNDCG: 0.005\n",
            "The time elapse of epoch 008 is: 00: 00: 11\n",
            "HR: 0.010\tNDCG: 0.010\n",
            "The time elapse of epoch 009 is: 00: 00: 12\n",
            "HR: 0.010\tNDCG: 0.010\n",
            "The time elapse of epoch 010 is: 00: 00: 12\n",
            "HR: 0.005\tNDCG: 0.005\n",
            "The time elapse of epoch 011 is: 00: 00: 12\n",
            "HR: 0.000\tNDCG: 0.000\n",
            "The time elapse of epoch 012 is: 00: 00: 12\n",
            "HR: 0.000\tNDCG: 0.000\n",
            "The time elapse of epoch 013 is: 00: 00: 12\n",
            "HR: 0.000\tNDCG: 0.000\n",
            "The time elapse of epoch 014 is: 00: 00: 12\n",
            "HR: 0.005\tNDCG: 0.005\n",
            "The time elapse of epoch 015 is: 00: 00: 12\n",
            "HR: 0.005\tNDCG: 0.005\n",
            "The time elapse of epoch 016 is: 00: 00: 12\n",
            "HR: 0.000\tNDCG: 0.000\n",
            "The time elapse of epoch 017 is: 00: 00: 12\n",
            "HR: 0.015\tNDCG: 0.015\n",
            "The time elapse of epoch 018 is: 00: 00: 12\n",
            "HR: 0.005\tNDCG: 0.005\n",
            "The time elapse of epoch 019 is: 00: 00: 12\n",
            "HR: 0.000\tNDCG: 0.000\n",
            "End. Best epoch 002: HR = 0.015, NDCG = 0.015\n",
            "Top_K 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The time elapse of epoch 000 is: 00: 00: 12\n",
            "HR: 0.030\tNDCG: 0.026\n",
            "The time elapse of epoch 001 is: 00: 00: 12\n",
            "HR: 0.025\tNDCG: 0.021\n",
            "The time elapse of epoch 002 is: 00: 00: 12\n",
            "HR: 0.020\tNDCG: 0.018\n",
            "The time elapse of epoch 003 is: 00: 00: 12\n",
            "HR: 0.020\tNDCG: 0.014\n",
            "The time elapse of epoch 004 is: 00: 00: 12\n",
            "HR: 0.015\tNDCG: 0.013\n",
            "The time elapse of epoch 005 is: 00: 00: 12\n",
            "HR: 0.010\tNDCG: 0.008\n",
            "The time elapse of epoch 006 is: 00: 00: 12\n",
            "HR: 0.015\tNDCG: 0.013\n",
            "The time elapse of epoch 007 is: 00: 00: 12\n",
            "HR: 0.010\tNDCG: 0.008\n",
            "The time elapse of epoch 008 is: 00: 00: 11\n",
            "HR: 0.005\tNDCG: 0.003\n",
            "The time elapse of epoch 009 is: 00: 00: 12\n",
            "HR: 0.010\tNDCG: 0.006\n",
            "The time elapse of epoch 010 is: 00: 00: 12\n",
            "HR: 0.015\tNDCG: 0.009\n",
            "The time elapse of epoch 011 is: 00: 00: 12\n",
            "HR: 0.005\tNDCG: 0.003\n",
            "The time elapse of epoch 012 is: 00: 00: 12\n",
            "HR: 0.015\tNDCG: 0.009\n",
            "The time elapse of epoch 013 is: 00: 00: 12\n",
            "HR: 0.015\tNDCG: 0.015\n",
            "The time elapse of epoch 014 is: 00: 00: 12\n",
            "HR: 0.015\tNDCG: 0.011\n",
            "The time elapse of epoch 015 is: 00: 00: 12\n",
            "HR: 0.010\tNDCG: 0.008\n",
            "The time elapse of epoch 016 is: 00: 00: 12\n",
            "HR: 0.015\tNDCG: 0.013\n",
            "The time elapse of epoch 017 is: 00: 00: 11\n",
            "HR: 0.010\tNDCG: 0.010\n",
            "The time elapse of epoch 018 is: 00: 00: 11\n",
            "HR: 0.010\tNDCG: 0.010\n",
            "The time elapse of epoch 019 is: 00: 00: 12\n",
            "HR: 0.015\tNDCG: 0.013\n",
            "End. Best epoch 000: HR = 0.030, NDCG = 0.026\n",
            "Top_K 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The time elapse of epoch 000 is: 00: 00: 12\n",
            "HR: 0.025\tNDCG: 0.018\n",
            "The time elapse of epoch 001 is: 00: 00: 12\n",
            "HR: 0.045\tNDCG: 0.037\n",
            "The time elapse of epoch 002 is: 00: 00: 12\n",
            "HR: 0.045\tNDCG: 0.030\n",
            "The time elapse of epoch 003 is: 00: 00: 12\n",
            "HR: 0.040\tNDCG: 0.028\n",
            "The time elapse of epoch 004 is: 00: 00: 12\n",
            "HR: 0.030\tNDCG: 0.022\n",
            "The time elapse of epoch 005 is: 00: 00: 12\n",
            "HR: 0.040\tNDCG: 0.029\n",
            "The time elapse of epoch 006 is: 00: 00: 12\n",
            "HR: 0.045\tNDCG: 0.029\n",
            "The time elapse of epoch 007 is: 00: 00: 12\n",
            "HR: 0.035\tNDCG: 0.024\n",
            "The time elapse of epoch 008 is: 00: 00: 11\n",
            "HR: 0.040\tNDCG: 0.024\n",
            "The time elapse of epoch 009 is: 00: 00: 12\n",
            "HR: 0.035\tNDCG: 0.022\n",
            "The time elapse of epoch 010 is: 00: 00: 12\n",
            "HR: 0.030\tNDCG: 0.019\n",
            "The time elapse of epoch 011 is: 00: 00: 12\n",
            "HR: 0.030\tNDCG: 0.019\n",
            "The time elapse of epoch 012 is: 00: 00: 12\n",
            "HR: 0.030\tNDCG: 0.018\n",
            "The time elapse of epoch 013 is: 00: 00: 12\n",
            "HR: 0.030\tNDCG: 0.018\n",
            "The time elapse of epoch 014 is: 00: 00: 12\n",
            "HR: 0.030\tNDCG: 0.019\n",
            "The time elapse of epoch 015 is: 00: 00: 12\n",
            "HR: 0.030\tNDCG: 0.019\n",
            "The time elapse of epoch 016 is: 00: 00: 12\n",
            "HR: 0.035\tNDCG: 0.022\n",
            "The time elapse of epoch 017 is: 00: 00: 12\n",
            "HR: 0.035\tNDCG: 0.024\n",
            "The time elapse of epoch 018 is: 00: 00: 12\n",
            "HR: 0.035\tNDCG: 0.026\n",
            "The time elapse of epoch 019 is: 00: 00: 11\n",
            "HR: 0.025\tNDCG: 0.016\n",
            "End. Best epoch 001: HR = 0.045, NDCG = 0.037\n",
            "Top_K 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The time elapse of epoch 000 is: 00: 00: 11\n",
            "HR: 0.040\tNDCG: 0.029\n",
            "The time elapse of epoch 001 is: 00: 00: 12\n",
            "HR: 0.040\tNDCG: 0.028\n",
            "The time elapse of epoch 002 is: 00: 00: 12\n",
            "HR: 0.055\tNDCG: 0.036\n",
            "The time elapse of epoch 003 is: 00: 00: 12\n",
            "HR: 0.040\tNDCG: 0.022\n",
            "The time elapse of epoch 004 is: 00: 00: 12\n",
            "HR: 0.050\tNDCG: 0.033\n",
            "The time elapse of epoch 005 is: 00: 00: 12\n",
            "HR: 0.040\tNDCG: 0.022\n",
            "The time elapse of epoch 006 is: 00: 00: 12\n",
            "HR: 0.040\tNDCG: 0.020\n",
            "The time elapse of epoch 007 is: 00: 00: 12\n",
            "HR: 0.025\tNDCG: 0.015\n",
            "The time elapse of epoch 008 is: 00: 00: 12\n",
            "HR: 0.025\tNDCG: 0.014\n",
            "The time elapse of epoch 009 is: 00: 00: 12\n",
            "HR: 0.025\tNDCG: 0.011\n",
            "The time elapse of epoch 010 is: 00: 00: 11\n",
            "HR: 0.015\tNDCG: 0.008\n",
            "The time elapse of epoch 011 is: 00: 00: 12\n",
            "HR: 0.015\tNDCG: 0.007\n",
            "The time elapse of epoch 012 is: 00: 00: 12\n",
            "HR: 0.010\tNDCG: 0.005\n",
            "The time elapse of epoch 013 is: 00: 00: 13\n",
            "HR: 0.005\tNDCG: 0.002\n",
            "The time elapse of epoch 014 is: 00: 00: 12\n",
            "HR: 0.015\tNDCG: 0.006\n",
            "The time elapse of epoch 015 is: 00: 00: 12\n",
            "HR: 0.020\tNDCG: 0.009\n",
            "The time elapse of epoch 016 is: 00: 00: 12\n",
            "HR: 0.030\tNDCG: 0.014\n",
            "The time elapse of epoch 017 is: 00: 00: 12\n",
            "HR: 0.030\tNDCG: 0.014\n",
            "The time elapse of epoch 018 is: 00: 00: 12\n",
            "HR: 0.040\tNDCG: 0.020\n",
            "The time elapse of epoch 019 is: 00: 00: 12\n",
            "HR: 0.030\tNDCG: 0.015\n",
            "End. Best epoch 002: HR = 0.055, NDCG = 0.036\n",
            "Top_K 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The time elapse of epoch 000 is: 00: 00: 12\n",
            "HR: 0.045\tNDCG: 0.027\n",
            "The time elapse of epoch 001 is: 00: 00: 12\n",
            "HR: 0.060\tNDCG: 0.032\n",
            "The time elapse of epoch 002 is: 00: 00: 12\n",
            "HR: 0.045\tNDCG: 0.029\n",
            "The time elapse of epoch 003 is: 00: 00: 12\n",
            "HR: 0.045\tNDCG: 0.028\n",
            "The time elapse of epoch 004 is: 00: 00: 12\n",
            "HR: 0.060\tNDCG: 0.034\n",
            "The time elapse of epoch 005 is: 00: 00: 12\n",
            "HR: 0.060\tNDCG: 0.035\n",
            "The time elapse of epoch 006 is: 00: 00: 12\n",
            "HR: 0.045\tNDCG: 0.030\n",
            "The time elapse of epoch 007 is: 00: 00: 12\n",
            "HR: 0.050\tNDCG: 0.026\n",
            "The time elapse of epoch 008 is: 00: 00: 13\n",
            "HR: 0.055\tNDCG: 0.033\n",
            "The time elapse of epoch 009 is: 00: 00: 13\n",
            "HR: 0.070\tNDCG: 0.038\n",
            "The time elapse of epoch 010 is: 00: 00: 13\n",
            "HR: 0.055\tNDCG: 0.027\n",
            "The time elapse of epoch 011 is: 00: 00: 12\n",
            "HR: 0.055\tNDCG: 0.025\n",
            "The time elapse of epoch 012 is: 00: 00: 12\n",
            "HR: 0.045\tNDCG: 0.023\n",
            "The time elapse of epoch 013 is: 00: 00: 12\n",
            "HR: 0.055\tNDCG: 0.025\n",
            "The time elapse of epoch 014 is: 00: 00: 13\n",
            "HR: 0.060\tNDCG: 0.029\n",
            "The time elapse of epoch 015 is: 00: 00: 13\n",
            "HR: 0.055\tNDCG: 0.027\n",
            "The time elapse of epoch 016 is: 00: 00: 12\n",
            "HR: 0.050\tNDCG: 0.028\n",
            "The time elapse of epoch 017 is: 00: 00: 12\n",
            "HR: 0.060\tNDCG: 0.030\n",
            "The time elapse of epoch 018 is: 00: 00: 12\n",
            "HR: 0.045\tNDCG: 0.025\n",
            "The time elapse of epoch 019 is: 00: 00: 12\n",
            "HR: 0.055\tNDCG: 0.028\n",
            "End. Best epoch 009: HR = 0.070, NDCG = 0.038\n",
            "Top_K 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The time elapse of epoch 000 is: 00: 00: 12\n",
            "HR: 0.050\tNDCG: 0.034\n",
            "The time elapse of epoch 001 is: 00: 00: 12\n",
            "HR: 0.070\tNDCG: 0.037\n",
            "The time elapse of epoch 002 is: 00: 00: 13\n",
            "HR: 0.060\tNDCG: 0.031\n",
            "The time elapse of epoch 003 is: 00: 00: 13\n",
            "HR: 0.085\tNDCG: 0.044\n",
            "The time elapse of epoch 004 is: 00: 00: 13\n",
            "HR: 0.075\tNDCG: 0.036\n",
            "The time elapse of epoch 005 is: 00: 00: 13\n",
            "HR: 0.070\tNDCG: 0.035\n",
            "The time elapse of epoch 006 is: 00: 00: 13\n",
            "HR: 0.080\tNDCG: 0.036\n",
            "The time elapse of epoch 007 is: 00: 00: 13\n",
            "HR: 0.070\tNDCG: 0.033\n",
            "The time elapse of epoch 008 is: 00: 00: 13\n",
            "HR: 0.060\tNDCG: 0.028\n",
            "The time elapse of epoch 009 is: 00: 00: 13\n",
            "HR: 0.055\tNDCG: 0.027\n",
            "The time elapse of epoch 010 is: 00: 00: 13\n",
            "HR: 0.045\tNDCG: 0.024\n",
            "The time elapse of epoch 011 is: 00: 00: 12\n",
            "HR: 0.055\tNDCG: 0.029\n",
            "The time elapse of epoch 012 is: 00: 00: 13\n",
            "HR: 0.045\tNDCG: 0.024\n",
            "The time elapse of epoch 013 is: 00: 00: 12\n",
            "HR: 0.050\tNDCG: 0.028\n",
            "The time elapse of epoch 014 is: 00: 00: 13\n",
            "HR: 0.060\tNDCG: 0.030\n",
            "The time elapse of epoch 015 is: 00: 00: 12\n",
            "HR: 0.065\tNDCG: 0.032\n",
            "The time elapse of epoch 016 is: 00: 00: 12\n",
            "HR: 0.070\tNDCG: 0.037\n",
            "The time elapse of epoch 017 is: 00: 00: 12\n",
            "HR: 0.050\tNDCG: 0.027\n",
            "The time elapse of epoch 018 is: 00: 00: 12\n",
            "HR: 0.055\tNDCG: 0.031\n",
            "The time elapse of epoch 019 is: 00: 00: 12\n",
            "HR: 0.060\tNDCG: 0.027\n",
            "End. Best epoch 003: HR = 0.085, NDCG = 0.044\n",
            "Top_K 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The time elapse of epoch 000 is: 00: 00: 13\n",
            "HR: 0.060\tNDCG: 0.030\n",
            "The time elapse of epoch 001 is: 00: 00: 12\n",
            "HR: 0.070\tNDCG: 0.041\n",
            "The time elapse of epoch 002 is: 00: 00: 13\n",
            "HR: 0.095\tNDCG: 0.045\n",
            "The time elapse of epoch 003 is: 00: 00: 13\n",
            "HR: 0.090\tNDCG: 0.040\n",
            "The time elapse of epoch 004 is: 00: 00: 13\n",
            "HR: 0.075\tNDCG: 0.040\n",
            "The time elapse of epoch 005 is: 00: 00: 13\n",
            "HR: 0.065\tNDCG: 0.034\n",
            "The time elapse of epoch 006 is: 00: 00: 13\n",
            "HR: 0.090\tNDCG: 0.041\n",
            "The time elapse of epoch 007 is: 00: 00: 14\n",
            "HR: 0.070\tNDCG: 0.033\n",
            "The time elapse of epoch 008 is: 00: 00: 13\n",
            "HR: 0.060\tNDCG: 0.031\n",
            "The time elapse of epoch 009 is: 00: 00: 13\n",
            "HR: 0.060\tNDCG: 0.027\n",
            "The time elapse of epoch 010 is: 00: 00: 13\n",
            "HR: 0.060\tNDCG: 0.028\n",
            "The time elapse of epoch 011 is: 00: 00: 13\n",
            "HR: 0.055\tNDCG: 0.023\n",
            "The time elapse of epoch 012 is: 00: 00: 13\n",
            "HR: 0.075\tNDCG: 0.032\n",
            "The time elapse of epoch 013 is: 00: 00: 13\n",
            "HR: 0.060\tNDCG: 0.026\n",
            "The time elapse of epoch 014 is: 00: 00: 13\n",
            "HR: 0.060\tNDCG: 0.026\n",
            "The time elapse of epoch 015 is: 00: 00: 13\n",
            "HR: 0.060\tNDCG: 0.027\n",
            "The time elapse of epoch 016 is: 00: 00: 14\n",
            "HR: 0.065\tNDCG: 0.029\n",
            "The time elapse of epoch 017 is: 00: 00: 14\n",
            "HR: 0.080\tNDCG: 0.035\n",
            "The time elapse of epoch 018 is: 00: 00: 13\n",
            "HR: 0.055\tNDCG: 0.025\n",
            "The time elapse of epoch 019 is: 00: 00: 13\n",
            "HR: 0.080\tNDCG: 0.035\n",
            "End. Best epoch 002: HR = 0.095, NDCG = 0.045\n",
            "Top_K 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The time elapse of epoch 000 is: 00: 00: 13\n",
            "HR: 0.095\tNDCG: 0.049\n",
            "The time elapse of epoch 001 is: 00: 00: 13\n",
            "HR: 0.125\tNDCG: 0.063\n",
            "The time elapse of epoch 002 is: 00: 00: 13\n",
            "HR: 0.100\tNDCG: 0.048\n",
            "The time elapse of epoch 003 is: 00: 00: 13\n",
            "HR: 0.105\tNDCG: 0.052\n",
            "The time elapse of epoch 004 is: 00: 00: 14\n",
            "HR: 0.100\tNDCG: 0.048\n",
            "The time elapse of epoch 005 is: 00: 00: 13\n",
            "HR: 0.100\tNDCG: 0.054\n",
            "The time elapse of epoch 006 is: 00: 00: 13\n",
            "HR: 0.105\tNDCG: 0.049\n",
            "The time elapse of epoch 007 is: 00: 00: 13\n",
            "HR: 0.090\tNDCG: 0.043\n",
            "The time elapse of epoch 008 is: 00: 00: 13\n",
            "HR: 0.095\tNDCG: 0.044\n",
            "The time elapse of epoch 009 is: 00: 00: 13\n",
            "HR: 0.075\tNDCG: 0.034\n",
            "The time elapse of epoch 010 is: 00: 00: 13\n",
            "HR: 0.090\tNDCG: 0.039\n",
            "The time elapse of epoch 011 is: 00: 00: 13\n",
            "HR: 0.105\tNDCG: 0.041\n",
            "The time elapse of epoch 012 is: 00: 00: 13\n",
            "HR: 0.075\tNDCG: 0.030\n",
            "The time elapse of epoch 013 is: 00: 00: 13\n",
            "HR: 0.080\tNDCG: 0.034\n",
            "The time elapse of epoch 014 is: 00: 00: 13\n",
            "HR: 0.070\tNDCG: 0.029\n",
            "The time elapse of epoch 015 is: 00: 00: 13\n",
            "HR: 0.085\tNDCG: 0.038\n",
            "The time elapse of epoch 016 is: 00: 00: 13\n",
            "HR: 0.075\tNDCG: 0.034\n",
            "The time elapse of epoch 017 is: 00: 00: 13\n",
            "HR: 0.070\tNDCG: 0.030\n",
            "The time elapse of epoch 018 is: 00: 00: 13\n",
            "HR: 0.065\tNDCG: 0.029\n",
            "The time elapse of epoch 019 is: 00: 00: 14\n",
            "HR: 0.090\tNDCG: 0.038\n",
            "End. Best epoch 001: HR = 0.125, NDCG = 0.063\n",
            "Top_K 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The time elapse of epoch 000 is: 00: 00: 13\n",
            "HR: 0.095\tNDCG: 0.048\n",
            "The time elapse of epoch 001 is: 00: 00: 13\n",
            "HR: 0.105\tNDCG: 0.046\n",
            "The time elapse of epoch 002 is: 00: 00: 13\n",
            "HR: 0.105\tNDCG: 0.047\n",
            "The time elapse of epoch 003 is: 00: 00: 13\n",
            "HR: 0.110\tNDCG: 0.049\n",
            "The time elapse of epoch 004 is: 00: 00: 13\n",
            "HR: 0.105\tNDCG: 0.049\n",
            "The time elapse of epoch 005 is: 00: 00: 13\n",
            "HR: 0.100\tNDCG: 0.046\n",
            "The time elapse of epoch 006 is: 00: 00: 13\n",
            "HR: 0.095\tNDCG: 0.045\n",
            "The time elapse of epoch 007 is: 00: 00: 13\n",
            "HR: 0.095\tNDCG: 0.045\n",
            "The time elapse of epoch 008 is: 00: 00: 14\n",
            "HR: 0.110\tNDCG: 0.049\n",
            "The time elapse of epoch 009 is: 00: 00: 13\n",
            "HR: 0.125\tNDCG: 0.050\n",
            "The time elapse of epoch 010 is: 00: 00: 13\n",
            "HR: 0.115\tNDCG: 0.047\n",
            "The time elapse of epoch 011 is: 00: 00: 13\n",
            "HR: 0.095\tNDCG: 0.042\n",
            "The time elapse of epoch 012 is: 00: 00: 13\n",
            "HR: 0.130\tNDCG: 0.053\n",
            "The time elapse of epoch 013 is: 00: 00: 13\n",
            "HR: 0.110\tNDCG: 0.047\n",
            "The time elapse of epoch 014 is: 00: 00: 13\n",
            "HR: 0.120\tNDCG: 0.050\n",
            "The time elapse of epoch 015 is: 00: 00: 13\n",
            "HR: 0.120\tNDCG: 0.051\n",
            "The time elapse of epoch 016 is: 00: 00: 13\n",
            "HR: 0.110\tNDCG: 0.049\n",
            "The time elapse of epoch 017 is: 00: 00: 13\n",
            "HR: 0.110\tNDCG: 0.048\n",
            "The time elapse of epoch 018 is: 00: 00: 13\n",
            "HR: 0.115\tNDCG: 0.047\n",
            "The time elapse of epoch 019 is: 00: 00: 13\n",
            "HR: 0.140\tNDCG: 0.058\n",
            "End. Best epoch 019: HR = 0.140, NDCG = 0.058\n",
            "Top_K 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The time elapse of epoch 000 is: 00: 00: 13\n",
            "HR: 0.085\tNDCG: 0.042\n",
            "The time elapse of epoch 001 is: 00: 00: 13\n",
            "HR: 0.100\tNDCG: 0.045\n",
            "The time elapse of epoch 002 is: 00: 00: 13\n",
            "HR: 0.115\tNDCG: 0.048\n",
            "The time elapse of epoch 003 is: 00: 00: 13\n",
            "HR: 0.115\tNDCG: 0.052\n",
            "The time elapse of epoch 004 is: 00: 00: 13\n",
            "HR: 0.120\tNDCG: 0.052\n",
            "The time elapse of epoch 005 is: 00: 00: 13\n",
            "HR: 0.120\tNDCG: 0.054\n",
            "The time elapse of epoch 006 is: 00: 00: 13\n",
            "HR: 0.115\tNDCG: 0.049\n",
            "The time elapse of epoch 007 is: 00: 00: 13\n",
            "HR: 0.090\tNDCG: 0.042\n",
            "The time elapse of epoch 008 is: 00: 00: 13\n",
            "HR: 0.080\tNDCG: 0.038\n",
            "The time elapse of epoch 009 is: 00: 00: 13\n",
            "HR: 0.095\tNDCG: 0.041\n",
            "The time elapse of epoch 010 is: 00: 00: 13\n",
            "HR: 0.105\tNDCG: 0.045\n",
            "The time elapse of epoch 011 is: 00: 00: 13\n",
            "HR: 0.090\tNDCG: 0.038\n",
            "The time elapse of epoch 012 is: 00: 00: 13\n",
            "HR: 0.075\tNDCG: 0.031\n",
            "The time elapse of epoch 013 is: 00: 00: 13\n",
            "HR: 0.085\tNDCG: 0.034\n",
            "The time elapse of epoch 014 is: 00: 00: 13\n",
            "HR: 0.085\tNDCG: 0.035\n",
            "The time elapse of epoch 015 is: 00: 00: 13\n",
            "HR: 0.070\tNDCG: 0.031\n",
            "The time elapse of epoch 016 is: 00: 00: 13\n",
            "HR: 0.080\tNDCG: 0.033\n",
            "The time elapse of epoch 017 is: 00: 00: 13\n",
            "HR: 0.100\tNDCG: 0.038\n",
            "The time elapse of epoch 018 is: 00: 00: 14\n",
            "HR: 0.085\tNDCG: 0.042\n",
            "The time elapse of epoch 019 is: 00: 00: 14\n",
            "HR: 0.095\tNDCG: 0.040\n",
            "End. Best epoch 004: HR = 0.120, NDCG = 0.052\n"
          ]
        }
      ],
      "source": [
        "for top_k in range(1,11):\n",
        "\tprint(\"Top_K\",top_k)\n",
        "\t############################## PREPARE DATASET ##########################\n",
        "\ttrain_data, test_data, user_num ,item_num, train_mat = load_all()\n",
        "\n",
        "\t# construct the train and test datasets\n",
        "\ttrain_dataset = NCFData(\n",
        "\t\t\ttrain_data, item_num, train_mat, num_ng, True)\n",
        "\ttest_dataset = NCFData(\n",
        "\t\t\ttest_data, item_num, train_mat, 0, False)\n",
        "\ttrain_loader = data.DataLoader(train_dataset,\n",
        "\t\t\tbatch_size=batch_size, shuffle=True, num_workers=4)\n",
        "\ttest_loader = data.DataLoader(test_dataset,\n",
        "\t\t\tbatch_size=test_num_ng+1, shuffle=False, num_workers=0)\n",
        "\n",
        "\t########################### CREATE MODEL #################################\n",
        "\tGMF_model = None\n",
        "\tMLP_model = None\n",
        "\n",
        "\tmodel = NCF(user_num, item_num, factor_num, num_layers, dropout, model, GMF_model, MLP_model)\n",
        "\tmodel.cuda()\n",
        "\tloss_function = nn.BCEWithLogitsLoss()\n",
        "\n",
        "\n",
        "\toptimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "\t# writer = SummaryWriter() # for visualization\n",
        "\n",
        "\t########################### TRAINING #####################################\n",
        "\tcount, best_hr = 0, 0\n",
        "\tfor epoch in range(epochs):\n",
        "\t\tmodel.train() # Enable dropout (if have).\n",
        "\t\tstart_time = time.time()\n",
        "\t\ttrain_loader.dataset.ng_sample()\n",
        "\n",
        "\t\tfor user, item, label in train_loader:\n",
        "\t\t\tuser = user.cuda()\n",
        "\t\t\titem = item.cuda()\n",
        "\t\t\tlabel = label.float().cuda()\n",
        "\n",
        "\t\t\tmodel.zero_grad()\n",
        "\t\t\tprediction = model(user, item)\n",
        "\t\t\tloss = loss_function(prediction, label)\n",
        "\t\t\tloss.backward()\n",
        "\t\t\toptimizer.step()\n",
        "\t\t\t# writer.add_scalar('data/loss', loss.item(), count)\n",
        "\t\t\tcount += 1\n",
        "\n",
        "\t\tmodel.eval()\n",
        "\t\tHR, NDCG = metrics(model, test_loader, top_k)\n",
        "\n",
        "\t\telapsed_time = time.time() - start_time\n",
        "\t\tprint(\"The time elapse of epoch {:03d}\".format(epoch) + \" is: \" + \n",
        "\t\t\t\ttime.strftime(\"%H: %M: %S\", time.gmtime(elapsed_time)))\n",
        "\t\tprint(\"HR: {:.3f}\\tNDCG: {:.3f}\".format(np.mean(HR), np.mean(NDCG)))\n",
        "\n",
        "\t\tif HR > best_hr:\n",
        "\t\t\tbest_hr, best_ndcg, best_epoch, best_model = HR, NDCG, epoch, model\n",
        "\t\t\tif out:\n",
        "\t\t\t\tif not os.path.exists(model_path):\n",
        "\t\t\t\t\tos.mkdir(model_path)\n",
        "\t\t\t\t# torch.save(model, '{}{}.pth'.format(model_path, model))\n",
        "\n",
        "\tprint(\"End. Best epoch {:03d}: HR = {:.3f}, NDCG = {:.3f}\".format(best_epoch, best_hr, best_ndcg))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_k=10 \n",
        "for num_ng in range(1,11):\n",
        "\tprint(\"Num_ng\",num_ng)\n",
        "\t############################## PREPARE DATASET ##########################\n",
        "\ttrain_data, test_data, user_num ,item_num, train_mat = load_all()\n",
        "\n",
        "\t# construct the train and test datasets\n",
        "\ttrain_dataset = NCFData(\n",
        "\t\t\ttrain_data, item_num, train_mat, num_ng, True)\n",
        "\ttest_dataset = NCFData(\n",
        "\t\t\ttest_data, item_num, train_mat, 0, False)\n",
        "\ttrain_loader = data.DataLoader(train_dataset,\n",
        "\t\t\tbatch_size=batch_size, shuffle=True, num_workers=4)\n",
        "\ttest_loader = data.DataLoader(test_dataset,\n",
        "\t\t\tbatch_size=test_num_ng+1, shuffle=False, num_workers=0)\n",
        "\n",
        "\t########################### CREATE MODEL #################################\n",
        "\tGMF_model = None\n",
        "\tMLP_model = None\n",
        "\n",
        "\tmodel = NCF(user_num, item_num, factor_num, num_layers, dropout, model, GMF_model, MLP_model)\n",
        "\tmodel.cuda()\n",
        "\tloss_function = nn.BCEWithLogitsLoss()\n",
        "\n",
        "\n",
        "\toptimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "\t# writer = SummaryWriter() # for visualization\n",
        "\n",
        "\t########################### TRAINING #####################################\n",
        "\tcount, best_hr = 0, 0\n",
        "\tfor epoch in range(epochs):\n",
        "\t\tmodel.train() # Enable dropout (if have).\n",
        "\t\tstart_time = time.time()\n",
        "\t\ttrain_loader.dataset.ng_sample()\n",
        "\n",
        "\t\tfor user, item, label in train_loader:\n",
        "\t\t\tuser = user.cuda()\n",
        "\t\t\titem = item.cuda()\n",
        "\t\t\tlabel = label.float().cuda()\n",
        "\n",
        "\t\t\tmodel.zero_grad()\n",
        "\t\t\tprediction = model(user, item)\n",
        "\t\t\tloss = loss_function(prediction, label)\n",
        "\t\t\tloss.backward()\n",
        "\t\t\toptimizer.step()\n",
        "\t\t\t# writer.add_scalar('data/loss', loss.item(), count)\n",
        "\t\t\tcount += 1\n",
        "\n",
        "\t\tmodel.eval()\n",
        "\t\tHR, NDCG = metrics(model, test_loader, top_k)\n",
        "\n",
        "\t\telapsed_time = time.time() - start_time\n",
        "\t\tprint(\"The time elapse of epoch {:03d}\".format(epoch) + \" is: \" + \n",
        "\t\t\t\ttime.strftime(\"%H: %M: %S\", time.gmtime(elapsed_time)))\n",
        "\t\tprint(\"HR: {:.3f}\\tNDCG: {:.3f}\".format(np.mean(HR), np.mean(NDCG)))\n",
        "\n",
        "\t\tif HR > best_hr:\n",
        "\t\t\tbest_hr, best_ndcg, best_epoch, best_model = HR, NDCG, epoch, model\n",
        "\t\t\tif out:\n",
        "\t\t\t\tif not os.path.exists(model_path):\n",
        "\t\t\t\t\tos.mkdir(model_path)\n",
        "\t\t\t\t# torch.save(model, '{}{}.pth'.format(model_path, model))\n",
        "\n",
        "\tprint(\"End. Best epoch {:03d}: HR = {:.3f}, NDCG = {:.3f}\".format(best_epoch, best_hr, best_ndcg))"
      ],
      "metadata": {
        "id": "cqCjMmmTSunC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d516341c-7a81-406d-d313-797f8c7df546"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num_ng 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The time elapse of epoch 000 is: 00: 00: 05\n",
            "HR: 0.085\tNDCG: 0.038\n",
            "The time elapse of epoch 001 is: 00: 00: 04\n",
            "HR: 0.095\tNDCG: 0.045\n",
            "The time elapse of epoch 002 is: 00: 00: 06\n",
            "HR: 0.090\tNDCG: 0.050\n",
            "The time elapse of epoch 003 is: 00: 00: 04\n",
            "HR: 0.105\tNDCG: 0.045\n",
            "The time elapse of epoch 004 is: 00: 00: 05\n",
            "HR: 0.105\tNDCG: 0.049\n",
            "The time elapse of epoch 005 is: 00: 00: 05\n",
            "HR: 0.115\tNDCG: 0.051\n",
            "The time elapse of epoch 006 is: 00: 00: 04\n",
            "HR: 0.105\tNDCG: 0.049\n",
            "The time elapse of epoch 007 is: 00: 00: 05\n",
            "HR: 0.090\tNDCG: 0.050\n",
            "The time elapse of epoch 008 is: 00: 00: 04\n",
            "HR: 0.120\tNDCG: 0.058\n",
            "The time elapse of epoch 009 is: 00: 00: 05\n",
            "HR: 0.115\tNDCG: 0.054\n",
            "The time elapse of epoch 010 is: 00: 00: 05\n",
            "HR: 0.100\tNDCG: 0.051\n",
            "The time elapse of epoch 011 is: 00: 00: 05\n",
            "HR: 0.080\tNDCG: 0.042\n",
            "The time elapse of epoch 012 is: 00: 00: 05\n",
            "HR: 0.085\tNDCG: 0.039\n",
            "The time elapse of epoch 013 is: 00: 00: 05\n",
            "HR: 0.080\tNDCG: 0.041\n",
            "The time elapse of epoch 014 is: 00: 00: 05\n",
            "HR: 0.070\tNDCG: 0.035\n",
            "The time elapse of epoch 015 is: 00: 00: 05\n",
            "HR: 0.085\tNDCG: 0.041\n",
            "The time elapse of epoch 016 is: 00: 00: 04\n",
            "HR: 0.085\tNDCG: 0.036\n",
            "The time elapse of epoch 017 is: 00: 00: 06\n",
            "HR: 0.075\tNDCG: 0.035\n",
            "The time elapse of epoch 018 is: 00: 00: 04\n",
            "HR: 0.080\tNDCG: 0.040\n",
            "The time elapse of epoch 019 is: 00: 00: 05\n",
            "HR: 0.090\tNDCG: 0.041\n",
            "End. Best epoch 008: HR = 0.120, NDCG = 0.058\n",
            "Num_ng 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The time elapse of epoch 000 is: 00: 00: 07\n",
            "HR: 0.105\tNDCG: 0.049\n",
            "The time elapse of epoch 001 is: 00: 00: 08\n",
            "HR: 0.095\tNDCG: 0.045\n",
            "The time elapse of epoch 002 is: 00: 00: 07\n",
            "HR: 0.100\tNDCG: 0.049\n",
            "The time elapse of epoch 003 is: 00: 00: 08\n",
            "HR: 0.105\tNDCG: 0.050\n",
            "The time elapse of epoch 004 is: 00: 00: 07\n",
            "HR: 0.120\tNDCG: 0.055\n",
            "The time elapse of epoch 005 is: 00: 00: 08\n",
            "HR: 0.105\tNDCG: 0.049\n",
            "The time elapse of epoch 006 is: 00: 00: 08\n",
            "HR: 0.105\tNDCG: 0.049\n",
            "The time elapse of epoch 007 is: 00: 00: 07\n",
            "HR: 0.100\tNDCG: 0.046\n",
            "The time elapse of epoch 008 is: 00: 00: 08\n",
            "HR: 0.100\tNDCG: 0.046\n",
            "The time elapse of epoch 009 is: 00: 00: 07\n",
            "HR: 0.125\tNDCG: 0.054\n",
            "The time elapse of epoch 010 is: 00: 00: 08\n",
            "HR: 0.110\tNDCG: 0.051\n",
            "The time elapse of epoch 011 is: 00: 00: 08\n",
            "HR: 0.100\tNDCG: 0.044\n",
            "The time elapse of epoch 012 is: 00: 00: 07\n",
            "HR: 0.095\tNDCG: 0.042\n",
            "The time elapse of epoch 013 is: 00: 00: 08\n",
            "HR: 0.090\tNDCG: 0.038\n",
            "The time elapse of epoch 014 is: 00: 00: 07\n",
            "HR: 0.115\tNDCG: 0.045\n",
            "The time elapse of epoch 015 is: 00: 00: 08\n",
            "HR: 0.100\tNDCG: 0.039\n",
            "The time elapse of epoch 016 is: 00: 00: 08\n",
            "HR: 0.105\tNDCG: 0.037\n",
            "The time elapse of epoch 017 is: 00: 00: 07\n",
            "HR: 0.105\tNDCG: 0.038\n",
            "The time elapse of epoch 018 is: 00: 00: 08\n",
            "HR: 0.105\tNDCG: 0.039\n",
            "The time elapse of epoch 019 is: 00: 00: 07\n",
            "HR: 0.110\tNDCG: 0.038\n",
            "End. Best epoch 009: HR = 0.125, NDCG = 0.054\n",
            "Num_ng 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The time elapse of epoch 000 is: 00: 00: 11\n",
            "HR: 0.105\tNDCG: 0.042\n",
            "The time elapse of epoch 001 is: 00: 00: 11\n",
            "HR: 0.120\tNDCG: 0.053\n",
            "The time elapse of epoch 002 is: 00: 00: 11\n",
            "HR: 0.085\tNDCG: 0.038\n",
            "The time elapse of epoch 003 is: 00: 00: 11\n",
            "HR: 0.110\tNDCG: 0.046\n",
            "The time elapse of epoch 004 is: 00: 00: 10\n",
            "HR: 0.090\tNDCG: 0.041\n",
            "The time elapse of epoch 005 is: 00: 00: 11\n",
            "HR: 0.085\tNDCG: 0.038\n",
            "The time elapse of epoch 006 is: 00: 00: 11\n",
            "HR: 0.075\tNDCG: 0.029\n",
            "The time elapse of epoch 007 is: 00: 00: 11\n",
            "HR: 0.080\tNDCG: 0.031\n",
            "The time elapse of epoch 008 is: 00: 00: 11\n",
            "HR: 0.100\tNDCG: 0.037\n",
            "The time elapse of epoch 009 is: 00: 00: 10\n",
            "HR: 0.105\tNDCG: 0.039\n",
            "The time elapse of epoch 010 is: 00: 00: 10\n",
            "HR: 0.090\tNDCG: 0.036\n",
            "The time elapse of epoch 011 is: 00: 00: 11\n",
            "HR: 0.105\tNDCG: 0.042\n",
            "The time elapse of epoch 012 is: 00: 00: 11\n",
            "HR: 0.090\tNDCG: 0.037\n",
            "The time elapse of epoch 013 is: 00: 00: 11\n",
            "HR: 0.090\tNDCG: 0.038\n",
            "The time elapse of epoch 014 is: 00: 00: 10\n",
            "HR: 0.075\tNDCG: 0.032\n",
            "The time elapse of epoch 015 is: 00: 00: 11\n",
            "HR: 0.070\tNDCG: 0.031\n",
            "The time elapse of epoch 016 is: 00: 00: 11\n",
            "HR: 0.075\tNDCG: 0.036\n",
            "The time elapse of epoch 017 is: 00: 00: 11\n",
            "HR: 0.075\tNDCG: 0.033\n",
            "The time elapse of epoch 018 is: 00: 00: 11\n",
            "HR: 0.070\tNDCG: 0.033\n",
            "The time elapse of epoch 019 is: 00: 00: 10\n",
            "HR: 0.075\tNDCG: 0.035\n",
            "End. Best epoch 001: HR = 0.120, NDCG = 0.053\n",
            "Num_ng 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The time elapse of epoch 000 is: 00: 00: 13\n",
            "HR: 0.095\tNDCG: 0.050\n",
            "The time elapse of epoch 001 is: 00: 00: 14\n",
            "HR: 0.110\tNDCG: 0.049\n",
            "The time elapse of epoch 002 is: 00: 00: 13\n",
            "HR: 0.090\tNDCG: 0.046\n",
            "The time elapse of epoch 003 is: 00: 00: 13\n",
            "HR: 0.115\tNDCG: 0.055\n",
            "The time elapse of epoch 004 is: 00: 00: 14\n",
            "HR: 0.115\tNDCG: 0.057\n",
            "The time elapse of epoch 005 is: 00: 00: 13\n",
            "HR: 0.130\tNDCG: 0.065\n",
            "The time elapse of epoch 006 is: 00: 00: 13\n",
            "HR: 0.130\tNDCG: 0.061\n",
            "The time elapse of epoch 007 is: 00: 00: 13\n",
            "HR: 0.120\tNDCG: 0.058\n",
            "The time elapse of epoch 008 is: 00: 00: 13\n",
            "HR: 0.105\tNDCG: 0.054\n",
            "The time elapse of epoch 009 is: 00: 00: 13\n",
            "HR: 0.115\tNDCG: 0.057\n",
            "The time elapse of epoch 010 is: 00: 00: 13\n",
            "HR: 0.115\tNDCG: 0.051\n",
            "The time elapse of epoch 011 is: 00: 00: 13\n",
            "HR: 0.095\tNDCG: 0.048\n",
            "The time elapse of epoch 012 is: 00: 00: 13\n",
            "HR: 0.115\tNDCG: 0.054\n",
            "The time elapse of epoch 013 is: 00: 00: 14\n",
            "HR: 0.120\tNDCG: 0.058\n",
            "The time elapse of epoch 014 is: 00: 00: 14\n",
            "HR: 0.105\tNDCG: 0.052\n",
            "The time elapse of epoch 015 is: 00: 00: 13\n",
            "HR: 0.125\tNDCG: 0.055\n",
            "The time elapse of epoch 016 is: 00: 00: 13\n",
            "HR: 0.110\tNDCG: 0.053\n",
            "The time elapse of epoch 017 is: 00: 00: 13\n",
            "HR: 0.095\tNDCG: 0.049\n",
            "The time elapse of epoch 018 is: 00: 00: 14\n",
            "HR: 0.105\tNDCG: 0.053\n",
            "The time elapse of epoch 019 is: 00: 00: 13\n",
            "HR: 0.100\tNDCG: 0.051\n",
            "End. Best epoch 005: HR = 0.130, NDCG = 0.065\n",
            "Num_ng 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The time elapse of epoch 000 is: 00: 00: 16\n",
            "HR: 0.105\tNDCG: 0.060\n",
            "The time elapse of epoch 001 is: 00: 00: 16\n",
            "HR: 0.090\tNDCG: 0.050\n",
            "The time elapse of epoch 002 is: 00: 00: 16\n",
            "HR: 0.110\tNDCG: 0.051\n",
            "The time elapse of epoch 003 is: 00: 00: 17\n",
            "HR: 0.095\tNDCG: 0.042\n",
            "The time elapse of epoch 004 is: 00: 00: 16\n",
            "HR: 0.105\tNDCG: 0.044\n",
            "The time elapse of epoch 005 is: 00: 00: 15\n",
            "HR: 0.110\tNDCG: 0.045\n",
            "The time elapse of epoch 006 is: 00: 00: 16\n",
            "HR: 0.110\tNDCG: 0.045\n",
            "The time elapse of epoch 007 is: 00: 00: 16\n",
            "HR: 0.120\tNDCG: 0.044\n",
            "The time elapse of epoch 008 is: 00: 00: 17\n",
            "HR: 0.095\tNDCG: 0.037\n",
            "The time elapse of epoch 009 is: 00: 00: 16\n",
            "HR: 0.115\tNDCG: 0.042\n",
            "The time elapse of epoch 010 is: 00: 00: 16\n",
            "HR: 0.090\tNDCG: 0.035\n",
            "The time elapse of epoch 011 is: 00: 00: 16\n",
            "HR: 0.090\tNDCG: 0.036\n",
            "The time elapse of epoch 012 is: 00: 00: 16\n",
            "HR: 0.090\tNDCG: 0.037\n",
            "The time elapse of epoch 013 is: 00: 00: 18\n",
            "HR: 0.095\tNDCG: 0.036\n",
            "The time elapse of epoch 014 is: 00: 00: 16\n",
            "HR: 0.105\tNDCG: 0.038\n",
            "The time elapse of epoch 015 is: 00: 00: 16\n",
            "HR: 0.085\tNDCG: 0.033\n",
            "The time elapse of epoch 016 is: 00: 00: 16\n",
            "HR: 0.110\tNDCG: 0.040\n",
            "The time elapse of epoch 017 is: 00: 00: 17\n",
            "HR: 0.100\tNDCG: 0.039\n",
            "The time elapse of epoch 018 is: 00: 00: 17\n",
            "HR: 0.115\tNDCG: 0.042\n",
            "The time elapse of epoch 019 is: 00: 00: 16\n",
            "HR: 0.095\tNDCG: 0.035\n",
            "End. Best epoch 007: HR = 0.120, NDCG = 0.044\n",
            "Num_ng 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The time elapse of epoch 000 is: 00: 00: 19\n",
            "HR: 0.120\tNDCG: 0.055\n",
            "The time elapse of epoch 001 is: 00: 00: 20\n",
            "HR: 0.095\tNDCG: 0.047\n",
            "The time elapse of epoch 002 is: 00: 00: 18\n",
            "HR: 0.100\tNDCG: 0.042\n",
            "The time elapse of epoch 003 is: 00: 00: 20\n",
            "HR: 0.110\tNDCG: 0.050\n",
            "The time elapse of epoch 004 is: 00: 00: 19\n",
            "HR: 0.115\tNDCG: 0.043\n",
            "The time elapse of epoch 005 is: 00: 00: 19\n",
            "HR: 0.125\tNDCG: 0.051\n",
            "The time elapse of epoch 006 is: 00: 00: 20\n",
            "HR: 0.110\tNDCG: 0.047\n",
            "The time elapse of epoch 007 is: 00: 00: 19\n",
            "HR: 0.125\tNDCG: 0.049\n",
            "The time elapse of epoch 008 is: 00: 00: 20\n",
            "HR: 0.120\tNDCG: 0.051\n",
            "The time elapse of epoch 009 is: 00: 00: 18\n",
            "HR: 0.120\tNDCG: 0.053\n",
            "The time elapse of epoch 010 is: 00: 00: 20\n",
            "HR: 0.110\tNDCG: 0.049\n",
            "The time elapse of epoch 011 is: 00: 00: 19\n",
            "HR: 0.110\tNDCG: 0.045\n",
            "The time elapse of epoch 012 is: 00: 00: 19\n",
            "HR: 0.130\tNDCG: 0.052\n",
            "The time elapse of epoch 013 is: 00: 00: 20\n",
            "HR: 0.115\tNDCG: 0.048\n",
            "The time elapse of epoch 014 is: 00: 00: 19\n",
            "HR: 0.130\tNDCG: 0.054\n",
            "The time elapse of epoch 015 is: 00: 00: 21\n",
            "HR: 0.125\tNDCG: 0.055\n",
            "The time elapse of epoch 016 is: 00: 00: 19\n",
            "HR: 0.120\tNDCG: 0.047\n",
            "The time elapse of epoch 017 is: 00: 00: 20\n",
            "HR: 0.130\tNDCG: 0.054\n",
            "The time elapse of epoch 018 is: 00: 00: 18\n",
            "HR: 0.155\tNDCG: 0.064\n",
            "The time elapse of epoch 019 is: 00: 00: 19\n",
            "HR: 0.125\tNDCG: 0.056\n",
            "End. Best epoch 018: HR = 0.155, NDCG = 0.064\n",
            "Num_ng 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The time elapse of epoch 000 is: 00: 00: 21\n",
            "HR: 0.105\tNDCG: 0.051\n",
            "The time elapse of epoch 001 is: 00: 00: 23\n",
            "HR: 0.095\tNDCG: 0.045\n",
            "The time elapse of epoch 002 is: 00: 00: 22\n",
            "HR: 0.105\tNDCG: 0.051\n",
            "The time elapse of epoch 003 is: 00: 00: 22\n",
            "HR: 0.100\tNDCG: 0.042\n",
            "The time elapse of epoch 004 is: 00: 00: 23\n",
            "HR: 0.095\tNDCG: 0.046\n",
            "The time elapse of epoch 005 is: 00: 00: 21\n",
            "HR: 0.095\tNDCG: 0.037\n",
            "The time elapse of epoch 006 is: 00: 00: 23\n",
            "HR: 0.090\tNDCG: 0.039\n",
            "The time elapse of epoch 007 is: 00: 00: 23\n",
            "HR: 0.100\tNDCG: 0.043\n",
            "The time elapse of epoch 008 is: 00: 00: 21\n",
            "HR: 0.120\tNDCG: 0.047\n",
            "The time elapse of epoch 009 is: 00: 00: 23\n",
            "HR: 0.115\tNDCG: 0.044\n",
            "The time elapse of epoch 010 is: 00: 00: 23\n",
            "HR: 0.115\tNDCG: 0.046\n",
            "The time elapse of epoch 011 is: 00: 00: 21\n",
            "HR: 0.120\tNDCG: 0.046\n",
            "The time elapse of epoch 012 is: 00: 00: 23\n",
            "HR: 0.100\tNDCG: 0.042\n",
            "The time elapse of epoch 013 is: 00: 00: 22\n",
            "HR: 0.115\tNDCG: 0.046\n",
            "The time elapse of epoch 014 is: 00: 00: 21\n",
            "HR: 0.120\tNDCG: 0.047\n",
            "The time elapse of epoch 015 is: 00: 00: 22\n",
            "HR: 0.110\tNDCG: 0.046\n",
            "The time elapse of epoch 016 is: 00: 00: 22\n",
            "HR: 0.115\tNDCG: 0.047\n",
            "The time elapse of epoch 017 is: 00: 00: 22\n",
            "HR: 0.105\tNDCG: 0.040\n",
            "The time elapse of epoch 018 is: 00: 00: 22\n",
            "HR: 0.095\tNDCG: 0.040\n",
            "The time elapse of epoch 019 is: 00: 00: 21\n",
            "HR: 0.110\tNDCG: 0.041\n",
            "End. Best epoch 008: HR = 0.120, NDCG = 0.047\n",
            "Num_ng 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The time elapse of epoch 000 is: 00: 00: 25\n",
            "HR: 0.105\tNDCG: 0.056\n",
            "The time elapse of epoch 001 is: 00: 00: 25\n",
            "HR: 0.130\tNDCG: 0.069\n",
            "The time elapse of epoch 002 is: 00: 00: 25\n",
            "HR: 0.120\tNDCG: 0.053\n",
            "The time elapse of epoch 003 is: 00: 00: 25\n",
            "HR: 0.105\tNDCG: 0.044\n",
            "The time elapse of epoch 004 is: 00: 00: 25\n",
            "HR: 0.110\tNDCG: 0.044\n",
            "The time elapse of epoch 005 is: 00: 00: 24\n",
            "HR: 0.100\tNDCG: 0.039\n",
            "The time elapse of epoch 006 is: 00: 00: 24\n",
            "HR: 0.125\tNDCG: 0.048\n",
            "The time elapse of epoch 007 is: 00: 00: 26\n",
            "HR: 0.115\tNDCG: 0.046\n",
            "The time elapse of epoch 008 is: 00: 00: 25\n",
            "HR: 0.125\tNDCG: 0.048\n",
            "The time elapse of epoch 009 is: 00: 00: 25\n",
            "HR: 0.130\tNDCG: 0.054\n",
            "The time elapse of epoch 010 is: 00: 00: 25\n",
            "HR: 0.130\tNDCG: 0.054\n",
            "The time elapse of epoch 011 is: 00: 00: 26\n",
            "HR: 0.125\tNDCG: 0.051\n",
            "The time elapse of epoch 012 is: 00: 00: 24\n",
            "HR: 0.115\tNDCG: 0.049\n",
            "The time elapse of epoch 013 is: 00: 00: 24\n",
            "HR: 0.100\tNDCG: 0.042\n",
            "The time elapse of epoch 014 is: 00: 00: 25\n",
            "HR: 0.100\tNDCG: 0.044\n",
            "The time elapse of epoch 015 is: 00: 00: 26\n",
            "HR: 0.110\tNDCG: 0.045\n",
            "The time elapse of epoch 016 is: 00: 00: 25\n",
            "HR: 0.115\tNDCG: 0.046\n",
            "The time elapse of epoch 017 is: 00: 00: 25\n",
            "HR: 0.115\tNDCG: 0.045\n",
            "The time elapse of epoch 018 is: 00: 00: 25\n",
            "HR: 0.120\tNDCG: 0.045\n",
            "The time elapse of epoch 019 is: 00: 00: 24\n",
            "HR: 0.110\tNDCG: 0.040\n",
            "End. Best epoch 001: HR = 0.130, NDCG = 0.069\n",
            "Num_ng 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The time elapse of epoch 000 is: 00: 00: 28\n",
            "HR: 0.110\tNDCG: 0.049\n",
            "The time elapse of epoch 001 is: 00: 00: 28\n",
            "HR: 0.105\tNDCG: 0.047\n",
            "The time elapse of epoch 002 is: 00: 00: 28\n",
            "HR: 0.105\tNDCG: 0.051\n",
            "The time elapse of epoch 003 is: 00: 00: 28\n",
            "HR: 0.120\tNDCG: 0.051\n",
            "The time elapse of epoch 004 is: 00: 00: 28\n",
            "HR: 0.110\tNDCG: 0.045\n",
            "The time elapse of epoch 005 is: 00: 00: 28\n",
            "HR: 0.125\tNDCG: 0.051\n",
            "The time elapse of epoch 006 is: 00: 00: 28\n",
            "HR: 0.100\tNDCG: 0.041\n",
            "The time elapse of epoch 007 is: 00: 00: 28\n",
            "HR: 0.095\tNDCG: 0.037\n",
            "The time elapse of epoch 008 is: 00: 00: 27\n",
            "HR: 0.110\tNDCG: 0.046\n",
            "The time elapse of epoch 009 is: 00: 00: 28\n",
            "HR: 0.105\tNDCG: 0.041\n",
            "The time elapse of epoch 010 is: 00: 00: 28\n",
            "HR: 0.120\tNDCG: 0.051\n",
            "The time elapse of epoch 011 is: 00: 00: 28\n",
            "HR: 0.095\tNDCG: 0.040\n",
            "The time elapse of epoch 012 is: 00: 00: 28\n",
            "HR: 0.110\tNDCG: 0.045\n",
            "The time elapse of epoch 013 is: 00: 00: 28\n",
            "HR: 0.100\tNDCG: 0.043\n",
            "The time elapse of epoch 014 is: 00: 00: 29\n",
            "HR: 0.090\tNDCG: 0.040\n",
            "The time elapse of epoch 015 is: 00: 00: 29\n",
            "HR: 0.100\tNDCG: 0.043\n",
            "The time elapse of epoch 016 is: 00: 00: 28\n",
            "HR: 0.105\tNDCG: 0.044\n",
            "The time elapse of epoch 017 is: 00: 00: 28\n",
            "HR: 0.110\tNDCG: 0.045\n",
            "The time elapse of epoch 018 is: 00: 00: 28\n",
            "HR: 0.125\tNDCG: 0.051\n",
            "The time elapse of epoch 019 is: 00: 00: 28\n",
            "HR: 0.110\tNDCG: 0.050\n",
            "End. Best epoch 005: HR = 0.125, NDCG = 0.051\n",
            "Num_ng 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The time elapse of epoch 000 is: 00: 00: 31\n",
            "HR: 0.110\tNDCG: 0.052\n",
            "The time elapse of epoch 001 is: 00: 00: 31\n",
            "HR: 0.125\tNDCG: 0.055\n",
            "The time elapse of epoch 002 is: 00: 00: 32\n",
            "HR: 0.135\tNDCG: 0.062\n",
            "The time elapse of epoch 003 is: 00: 00: 32\n",
            "HR: 0.140\tNDCG: 0.058\n",
            "The time elapse of epoch 004 is: 00: 00: 31\n",
            "HR: 0.115\tNDCG: 0.051\n",
            "The time elapse of epoch 005 is: 00: 00: 32\n",
            "HR: 0.115\tNDCG: 0.051\n",
            "The time elapse of epoch 006 is: 00: 00: 31\n",
            "HR: 0.110\tNDCG: 0.046\n",
            "The time elapse of epoch 007 is: 00: 00: 31\n",
            "HR: 0.100\tNDCG: 0.042\n",
            "The time elapse of epoch 008 is: 00: 00: 32\n",
            "HR: 0.130\tNDCG: 0.052\n",
            "The time elapse of epoch 009 is: 00: 00: 31\n",
            "HR: 0.115\tNDCG: 0.046\n",
            "The time elapse of epoch 010 is: 00: 00: 31\n",
            "HR: 0.115\tNDCG: 0.046\n",
            "The time elapse of epoch 011 is: 00: 00: 31\n",
            "HR: 0.115\tNDCG: 0.048\n",
            "The time elapse of epoch 012 is: 00: 00: 32\n",
            "HR: 0.115\tNDCG: 0.047\n",
            "The time elapse of epoch 013 is: 00: 00: 31\n",
            "HR: 0.125\tNDCG: 0.049\n",
            "The time elapse of epoch 014 is: 00: 00: 31\n",
            "HR: 0.130\tNDCG: 0.051\n",
            "The time elapse of epoch 015 is: 00: 00: 33\n",
            "HR: 0.110\tNDCG: 0.047\n",
            "The time elapse of epoch 016 is: 00: 00: 31\n",
            "HR: 0.105\tNDCG: 0.046\n",
            "The time elapse of epoch 017 is: 00: 00: 32\n",
            "HR: 0.120\tNDCG: 0.049\n",
            "The time elapse of epoch 018 is: 00: 00: 33\n",
            "HR: 0.110\tNDCG: 0.047\n",
            "The time elapse of epoch 019 is: 00: 00: 30\n",
            "HR: 0.110\tNDCG: 0.047\n",
            "End. Best epoch 003: HR = 0.140, NDCG = 0.058\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}